\documentclass[11pt]{article}
\usepackage[left=2cm, right=2cm, top=2cm]{geometry}
\usepackage{amsmath}
\usepackage{listings}

\lstset{language=python}

\begin{document}
\title{Foundations}
\author{Prabhjot Singh Rai}
\maketitle

\section{Optimization and probability}

\textbf{Solution 1a:} In order to check whether the minima exists for the function, $f(\theta)$, we calculate $f''(\theta)$.
\begin{equation*}
\begin{split}
\frac{df(\theta)}{d\theta} & = \sum_{i=1}^{n} w_i(\theta - x_i)\\
g(\theta)& = \sum_{i=1}^{n} w_i \theta - \sum_{i=1}^{n} w_i x_i
\end{split}
\end{equation*}
Differentiating $g(\theta)$, we get
\begin{equation}
\frac{dg(\theta)}{d\theta} = \sum_{i=1}^{n} w_i \\
\end{equation}
Since it is given that $w_i, ...., w_n$ are positive real numbers, therefore $f''(\theta) > 0$, which proves that the graph of this quadratic function is convex and minima exists at value of $\theta$ when $f'(\theta) = 0$.
\begin{equation*}\
\begin{split}
\frac{df(\theta)}{d\theta} & =  \sum_{i=1}^{n} w_i(\theta - x_i)\\
0 & = \sum_{i=1}^{n} w_i \theta - \sum_{i=1}^{n} w_i x_i\\
\theta & = \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}
\end{split}
\end{equation*}
Therefore, the value of $\theta$ which minimizes $f(\theta)$ is $ \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}$.
If some of the $w_i$'s are negative, then from equation (1), it's not necessary that $f''(\theta)$ is always positive.If $f''(\theta)<0$, the function will be a concave function and minima will not be defined. And if a point of inflection exists ($f''(\theta) = 0$), then the function would be a straight line and minima won't be defined. \\
\linebreak
\textbf{Solution 1b.} Let $n$ vectors out of $x_i, .... x_d$ be negative where $n <= d$. Let sum of negative vectors be $-\beta$ and sum of positive vectors be $\alpha$, where $\alpha$ and $\beta$ are both positive.
Evaluating $f(x)$,
\begin{align*}
f(x) & = \sum_{i=1}^{d} max_{s\epsilon\{1,-1\}} sx_i \\
& =  max_{s\epsilon\{1,-1\}} s(\alpha) + max_{s\epsilon\{1,-1\}} s(-\beta)\\
& =  \alpha + \beta && \text{$max_{s\epsilon\{1,-1\}}$ is 1 for $\alpha$ and -1 for $\beta$)}\\
\end{align*}
\linebreak
Evaluating $g(x)$,
\begin{equation*}\
\begin{split}
g(x) & = max_{s\epsilon\{1,-1\}}\sum_{i=1}^{d} sx_i \\
& = max_{s\epsilon\{1,-1\}}s\sum_{i=1}^{d} x_i \\
& = max_{s\epsilon\{1,-1\}}s(\alpha - \beta)\\
g(x) & = \begin{cases}
\alpha - \beta & \text{for } \alpha > \beta \\
\beta - \alpha & \text{for } \beta > \alpha \\
0 & \text{for } \alpha = \beta \\
\end{cases}
\end{split}
\end{equation*}
Comparing $f(x)$ and $g(x)$ for different cases:
\begin{equation*}\
\begin{split}
f(x) - g(x) & = \begin{cases}
2\beta & \text{for } \alpha > \beta \\
2\alpha & \text{for } \beta > \alpha \\
2\alpha = 2\beta & \text{for } \beta = \alpha \\
\end{cases}
\end{split}
\end{equation*}
Therefore, if $\alpha$ and $\beta$ are non-zero, $f(x) > g(x)$, else $f(x) = g(x)$. Hence, for all \textbf{x}, $f(x) \geq g(x)$. \\
\linebreak
\textbf{Solution 1c.} Let $E(a, b)$ donate the expected number of points when we stop. Only cases to consider would be 1,2 and 6. Let's divide the sequences into 3 cases. \\
One third times we get a 1 first, and then we stop. \\
One third times we get a 2 first, and then we repeat. \\
One third times we get a 6 first, and then we repeat. \\
\linebreak
Expressing in equation form, it would be \\
\begin{equation*}\
\begin{split}
E(x) & = \frac{1}{3}(0) + \frac{1}{3}(-a + E(x)) + \frac{1}{3}(b + E(x)) \\
3 E(x) & = -a + b + 2E(x) \\
E(x) & = b-a
\end{split}
\end{equation*}
\linebreak
\textbf{Solution 1d.} Since we know that value of p for which $log L(p) = 0$ must also satisfy $L(p) = 0$, checking the point where the slope of the function is zero  \\
\begin{equation*}
\begin{split}
\log L(p) & = 4\log p + 3\log (1-p)\\
\frac{d \log L(p)}{d p} & = \frac{4}{p} - \frac{3}{1-p}\\
0 & = - 4 + 4p + 3p\\
p & = \frac{4}{7}
\end{split}
\end{equation*}
In order to check if it's a maxima or a minima at $ p = \frac{4}{7}$, let's check the sign on substituting p in $L''(p)$ \\
\begin{equation*}
\begin{split}
\frac{d L(p)}{d p} & = 4x^3(1-x)^3 - 3x^4(1-x)^2 \\
\frac{d^2 L(p)}{d p^2} & = (1-x)^2(12x^2 - 28x^3) - 2(1-x)(4x^3 - 7x^4) \\
\end{split}
\end{equation*}
Substituting $p= \frac{4}{7}$ we get an approximate value of $-0.238$, therefore, the function has a maxima at this point.\\
Intuitively, the maximum probability would be when $p$ is equivalent to the number of times the desired result came up divided by the total number of events. \\
\linebreak
\textbf{Solution 1e.} \\
\begin{align*}
f(w) & = \sum_{i=1}^{n}\sum_{j=1}^{n} (a_i^\top w - b_j^\top w)^2 + \lambda||w||_2^2 \\
& = \sum_{i=1}^{n}\sum_{j=1}^{n} ((a_i - b_j)^\top w)^2 + \lambda \sum_{k=1}^{d} w_k^2 && \text{(...using distributive property)} \\ 
\text{Calculating $f'(w)$,} \\
f'(w) & = 2\sum_{i=1}^{n}\sum_{j=1}^{n} ((a_i^\top - b_j^\top)w) (a_i^\top - b_j^\top) + \lambda 2w \\
\end{align*}

Thus, the gradient $\nabla f(w)$ can be expressed as
\begin{align*}
\nabla f(w) & = (\frac{\partial f(w)}{\partial w_1}, ... \frac{\partial f(w)}{\partial w_d})^\top \\
& = (2\sum_{i=1}^{n}\sum_{j=1}^{n} ((a_i^\top - b_j^\top)w_1) (a_i^\top - b_j^\top) + \lambda 2w_1, \dots 2\sum_{i=1}^{n}\sum_{j=1}^{n} ((a_i^\top - b_j^\top)w_d) (a_i^\top - b_j^\top) + \lambda 2w_d) \\ \\
\end{align*}
\newpage

\section{Complexity}

\textbf{Solution 2a.} This problem can be thought of creating a rectangle inside a rectangle of width $m$ pixels and height $n$ pixels ($m = n$ for this problem, but we will use it later). Therefore, the number of points which can be selected to create a rectangle are $m+1$ and $n+1$. \\
\linebreak
Total number of points  = (Number of ways first point is selected) (Number of ways next point is selected on the remaining points on width) (Number of ways next point is selected on remaining points on height) / (Number of points of rectangle, since for each point, the remaining points are selected again)
\begin{align*}
\text{No of ways} &= \frac{{(m+1)(n+1)\choose 1} {m\choose 1}{n\choose 1}}{4} \\
&= \frac{m(m+1)n(n+1)}{4}
\end{align*}
For the given problem, $m = n$. Therefore
\begin{align*}
\text{No of ways 1 rectange can be created} &= \frac{n^2(n+1)^2}{4}
\end{align*}
This way, the number of possible faces are \\
\begin{align*}
\text{No of ways 6 rectange can be created} &= (\frac{n^2(n+1)^2}{4})^6
\end{align*}
Therefore, asymptotic complexity for this function is $O(n^{24})$. \\
\linebreak
\textbf{Solution 2b.} Let $v(i, j)$ represent the cost at a particular point. Cost of touching the point $(i, j)$ can be defined as the sum of cost at $(i, j)$ and (cost of touching point at $(i-1, j)$ or cost of touching point at $(i, j-1)$). $$c(i, j) = v(i, j) + c(i-1, j) \textbf{ or } c(i, j) = v(i,j) + c(i, j-1)$$
Minimum cost to reach point $(i, j)$ is can be defined as \\
\begin{align*}
c_{min}(i, j) = v(i, j) + \min_{(i, j) \epsilon \{(i-1, j), (i, j-1)\}}c_{min}(i, j)
\end{align*}
where $\min_{(i, j) \epsilon \{(i-1, j), (i, j-1)\}}c_{min}(i, j)$ is the minimum of the cost between reaching point $(i-1, j)$ and $(i, j-1)$. \\
Let \lstinline{cost_matrix} be a matrix consisting of costs at all the points. Algorithm to compute this efficiently is defined as follows\\
\begin{lstlisting}[basicstyle=\small]
def min_cost(i, j, cost_matrix):

    # iterating over first column and computing cost
    # for traversing till the element (i, 0)
    for m in range(1, i + 1):
        cost_matrix[m][0] = cost_matrix[m - 1][0] + cost_matrix[m][0]

    # iterating over first row and computing cost
    # for traversing till the element (j, 0)
    for n in range(1, j + 1):
        cost_matrix[0][n] = cost_matrix[0][n - 1] + cost_matrix[0][n]

    # iterating over remaining entries for which the
    # cost for top and left elements have already
    # been computed
    for m in range(1, i + 1):
        for n in range(1, j + 1):
            cost_matrix[m][n] = min(cost_matrix[m - 1][n], cost_matrix[m][n - 1]) \
                                      + cost_matrix[m][n]

    return cost_matrix[i][j]
\end{lstlisting}
Since there are two nested loops for each dimension, the runtime of the function is $O(n^2)$.\\
\linebreak
\textbf{Solution 2c.} Total number of ways to reach to the top can be calculated by counting the number of ways the person can select a step on moving forward towards his way to the top, given that he has to land on the final step. \\
\linebreak
There can be two different cases for each step, either the step is selected on the way to the top or is skipped. \\
\begin{align*}
\text{Total ways} & = (\text{1st Selected or Skipped}) (\text{2nd Selected or Skipped}) \dots (\text{Last Selected}) \\ 
&= 2 * 2 * 2 \dots 1 \\
& = 2^{n-1}
\end{align*}
Therefore, total ways are $2^{n-1}$.\\
\linebreak
\textbf{Solution 2d.} 


\end{document}