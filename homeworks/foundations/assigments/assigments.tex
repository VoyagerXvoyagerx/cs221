\documentclass[11pt]{article}
\usepackage{amsmath}

\begin{document}
\title{Foundations}
\author{Prabhjot Singh Rai}
\maketitle

\section{Optimization and probability}

\textbf{Solution 1a:} In order to check whether the minima exists for the function, $f(\theta)$, we calculate $f''(\theta)$.
\begin{equation*}
\begin{split}
\frac{df(\theta)}{d\theta} & = \sum_{i=1}^{n} w_i(\theta - x_i)\\
g(\theta)& = \sum_{i=1}^{n} w_i \theta - \sum_{i=1}^{n} w_i x_i
\end{split}
\end{equation*}
Differentiating $g(\theta)$, we get
\begin{equation}
\frac{dg(\theta)}{d\theta} = \sum_{i=1}^{n} w_i \\
\end{equation}
Since it is given that $w_i, ...., w_n$ are positive real numbers, therefore $f''(\theta) > 0$, which shows that the graph of this quadratic function is concave up and minima exists at value of $\theta$ when $f'(\theta) = 0$.
\begin{equation*}\
\begin{split}
\frac{df(\theta)}{d\theta} & =  \sum_{i=1}^{n} w_i(\theta - x_i)\\
0 & = \sum_{i=1}^{n} w_i \theta - \sum_{i=1}^{n} w_i x_i\\
\theta & = \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}
\end{split}
\end{equation*}
Therefore, the value of $\theta$ which minimizes $f(\theta)$ is $ \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}$.
If some of the $w_i$'s are negative, then from equation (1), it's not necessary that $f''(\theta)$ is always positive.If $f''(\theta)<0$, the function may not have any defined minima, since it will be concave downwards. And if a point of inflection exists ($f''(\theta) = 0$), then again, there would be local minima and maxima but no defined global minima for the function. \\
\linebreak
\textbf{Solution 1b.} Let $n$ vectors out of $x_i, .... x_d$ be negative where $n <= d$. Let sum of negative vectors be $-\beta$ and sum of positive vectors be $\alpha$, where $\alpha$ and $\beta$ are both positive vectors.
Evaluating $f(x)$,
\begin{equation*}\
\begin{split}
f(x) & = \sum_{i=1}^{d} max_{s\epsilon\{1,-1\}} sx_i \\
& =  max_{s\epsilon\{1,-1\}} s(\alpha) + max_{s\epsilon\{1,-1\}} s(-\beta)\\
& =  \alpha + \beta\\
\end{split}
\end{equation*}
\linebreak
Evaluating $g(x)$,
\begin{equation*}\
\begin{split}
g(x) & = max_{s\epsilon\{1,-1\}}\sum_{i=1}^{d} sx_i \\
& = max_{s\epsilon\{1,-1\}}s\sum_{i=1}^{d} x_i \\
& = max_{s\epsilon\{1,-1\}}s(\alpha - \beta)\\
g(x) & = \begin{cases}
\alpha - \beta & \text{for } \alpha > \beta \\
\beta - \alpha & \text{for } \beta > \alpha \\
0 & \text{for } \alpha = \beta \\
\end{cases}
\end{split}
\end{equation*}
Comparing $f(x)$ and $g(x)$ for different cases:
\begin{equation*}\
\begin{split}
f(x) - g(x) & = \begin{cases}
2\beta & \text{for } \alpha > \beta \\
2\alpha & \text{for } \beta > \alpha \\
2\alpha = 2\beta & \text{for } \beta = \alpha \\
\end{cases}
\end{split}
\end{equation*}
Therefore, if $\alpha$ and $\beta$ are non-zero, $f(x) > g(x)$, else $f(x) = g(x)$. Hence, for all \textbf{x}, $f(x) \geq g(x)$. \\
\linebreak
\textbf{Solution 1c.} Let $E(a, b)$ donate the expected number of points when we stop. Only cases to consider would be 1,2 and 6. Let's divide the sequences into 3 cases. \\
One third times we get a 1 first, and then we stop. \\
One third times we get a 2 first, and then we repeat. \\
One third times we get a 6 first, and then we repeat. \\
\linebreak
Expressing in equation form, it would be \\
\begin{equation*}\
\begin{split}
E(x) & = \frac{1}{3}(0) + \frac{1}{3}(-a + E(x)) + \frac{1}{3}(b + E(x)) \\
3 E(x) & = -a + b + 2E(x) \\
E(x) & = b-a
\end{split}
\end{equation*}
\linebreak
\textbf{Solution 1d.} Since we know that value of p for which $log L(p) = 0$ must also satisfy $L(p) = 0$, checking the point where the slope of the function is zero  \\
\begin{equation*}
\begin{split}
\log L(p) & = 4\log p + 3\log (1-p)\\
\frac{d \log L(p)}{d p} & = \frac{4}{p} - \frac{3}{1-p}\\
0 & = - 4 + 4p + 3p\\
p & = \frac{4}{7}
\end{split}
\end{equation*}
In order to check if it's a maxima or a minima at $ p = \frac{4}{7}$, let's check the sign on substituting p in $L''(p)$ \\
\begin{equation*}
\begin{split}
\frac{d \log L(p)}{d p} & = p^3 (1-p)^2 (1-7p)\\
\frac{d^2 \log L(p)}{d p} & = - \frac{4}{p^2} + \frac{3}{(1-p)^2}\\
\end{split}
\end{equation*}
Substituting $p= \frac{4}{7}$ we get a negative number, therefore, the function has a maxima at this point.\\
Intuitively, the maximum probability of an event occurring is equivalent to the number of times the desired result came up divided by the total number of events. \\
\linebreak
\textbf{Solution 1e.} \\
\begin{align*}
f(w) & = \sum_{i=1}^{n}\sum_{j=1}^{n} (a_i^\top w - b_j^\top w)^2 + \lambda||w||_2^2 \\
& = \sum_{i=1}^{n}\sum_{j=1}^{n} (a_i^\top - b_j^\top)^2 w^2 + \lambda||w||_2^2 && \text{(...using distributive property)} \\ 
& = ||w||_2^2 \sum_{i=1}^{n}\sum_{j=1}^{n} (a_i^\top - b_j^\top)^2 + \lambda||w||_2^2 && \text{(...using $v*v = |v|^2$)} \\
& = ||w||_2^2 (\sum_{i=1}^{n}\sum_{j=1}^{n} (a_i^\top - b_j^\top)^2 + \lambda)
\end{align*}
Let $\alpha = \sum_{i=1}^{n}\sum_{j=1}^{n} (a_i^\top - b_j^\top)^2$, therefore \\
\begin{align*}
f(w) & = ||w||_2^2 (\alpha + \lambda) \\
& = (\sum_{k=1}^{d} w_k^2)(\alpha + \lambda)
\end{align*}
Thus, the gradient $\nabla f(w)$ can be expressed as
\begin{align*}
\nabla f(w) & = (\frac{\partial f(w)}{\partial w_1}, ... \frac{\partial f(w)}{\partial w_d})^\top \\
& = (2w_1(\alpha + \lambda), ... 2w_d (\alpha + \lambda)) && \text{where } \alpha = \sum_{i=1}^{n}\sum_{j=1}^{n} (a_i^\top - b_j^\top)^2
\end{align*}
\end{document}